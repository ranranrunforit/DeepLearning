{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2b-ZK9tuGmoK"
   },
   "source": [
    "* Making use of embeddings in Tensorflow\n",
    "* Coding CNNs in TF\n",
    "* Intuitions behind working of CNN\n",
    "* Intuitions behind embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Zjqi3WjaGmoL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chaor\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function,division\n",
    "from google.colab import files\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWzfE86tusct"
   },
   "source": [
    "## Sentiment Classification - dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atbvZSXlGmoQ"
   },
   "source": [
    "We will use movie review dataset taken from http://www.cs.cornell.edu/people/pabo/movie-review-data/. The exact dataset we will use is the Sentence-polarity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6l_qzOlhGmoR",
    "outputId": "1f2332b1-06fd-40ec-ddd3-225bc11ebb7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews of class_neg = 5331\n",
      "\tMax number of tokens in a sentence = 56\n",
      "\tMin number of tokens in a sentence = 1\n",
      "Number of reviews of class_pos = 5331\n",
      "\tMax number of tokens in a sentence = 59\n",
      "\tMin number of tokens in a sentence = 2\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for file_,label in zip([\"class_neg.txt\",\"class_pos.txt\"],[0,1]):\n",
    "    lines = open(file_).readlines()\n",
    "    lines = map(lambda x:x.strip().replace(\"-\",\" \").split(),lines)\n",
    "    for line in lines:\n",
    "        data.append([line,label])\n",
    "    print(\"Number of reviews of {} = {}\".format(file_[:-4],len(lines)))\n",
    "    print(\"\\tMax number of tokens in a sentence = {}\".format(max(map(lambda x:len(x),lines))))\n",
    "    print(\"\\tMin number of tokens in a sentence = {}\".format(min(map(lambda x:len(x),lines))))\n",
    "random.Random(5).shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKqVN0XTGmoV"
   },
   "source": [
    "Observe that the lengths of sentences are different. In case, we need to vectorize the operations, we need all sentences to be of equal length. Therefore, we will pad all sentences to be of equal length and substitute the padded parts of sentence with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UJSIi3EOGmoV",
    "outputId": "179ed366-f127-4820-80b6-190a3d8fad05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a movie that's held captive by mediocrity . not bad , but not all that good . bacon keeps things interesting , but don't go out of your way to pay full price .\n"
     ]
    }
   ],
   "source": [
    "# See some randomly sampled sentences\n",
    "print(\" \".join(data[random.randint(0,len(data))][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x70n4lA7GmoY"
   },
   "source": [
    "We will work with the sentence as given and not remove any stop-words or punctuation marks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "959vht4CGmoZ",
    "outputId": "5ead240b-6bd1-4bfb-e48c-125902935212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words :  19757\n",
      "train\n",
      "\tNumber of positive examples :  4288\n",
      "\tNumber of negative examples :  4241\n",
      "test\n",
      "\tNumber of positive examples :  1043\n",
      "\tNumber of negative examples :  1090\n"
     ]
    }
   ],
   "source": [
    "sents = map(lambda x:x[0],data) # all sentences\n",
    "all_words = set()\n",
    "for sent in sents:\n",
    "    all_words |= set(sent)\n",
    "all_words = sorted(list(all_words))\n",
    "vocab = {all_words[i]:i for i in range(len(all_words))}\n",
    "print(\"Number of words : \",len(vocab))\n",
    "train = data[:int(0.8*len(data))]\n",
    "test = data[int(0.8*len(data)):]\n",
    "train_data = []\n",
    "train_targets = []\n",
    "test_data = []\n",
    "test_targets = []\n",
    "for list_all,list_data,list_target,label_list in zip([train,test],[train_data,test_data],[train_targets,test_targets],[\"train\",\"test\"]):\n",
    "    for datum,label in list_all:\n",
    "        list_data.append([vocab[w] for w in datum])\n",
    "        list_target.append([label])\n",
    "    print(label_list)\n",
    "    print(\"\\tNumber of positive examples : \",list_target.count([1]))\n",
    "    print(\"\\tNumber of negative examples : \",list_target.count([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5NZIsmiGmoc"
   },
   "source": [
    "For implementation purposes, we will need an index for the padded word and we will use the index 19757.\n",
    "Note: For a dataset of this <i>small</i> size, we will need to do K-Fold Cross-validation to evaluate the performance. However, we will work with this train-test split for the rest of this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIWGatDXGmoc"
   },
   "source": [
    "## Simple Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQEg38m6Gmod"
   },
   "source": [
    "<img src=\"https://web.cs.dal.ca/~sastry/cnn_simple.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCvl-t5EGmoe"
   },
   "source": [
    "The above image shows the architecture of the simple model that we will implement for text classification. We are interested in the following hyperparameters apart from the number of filters (which we will set to 1 for this problem):\n",
    "* The span of the filter/the number of words considered for making the prediction.\n",
    "* The size of the stride.\n",
    "* The number of activations selected for feeding into softmax classifier.\n",
    "\n",
    "Before continuing:\n",
    "\n",
    "* Can you reason how the machine is classifying (in the above example)? The values of the activations are color-coded. Is this the only possible way the machine can work? \n",
    "\n",
    "    (Your answer might look like : ... filter is ... template matching ... )\n",
    "\n",
    "<b>Answer</b>:\n",
    "</br><b>As in [2] the filter is applied to a window of words to create a new feature. This window iterates over the entire input vector. It then selects the 2 strongest features to be used in a softmax classifier before back-propigating the error to update weights and biases. In this case it looks like \"I like this movie\" and \"this movie very much\" are considered strong features for (positive) classification. </b>\n",
    "</br><b>Another approach to this machine could be to have multiple filters of various sizes. The largest features of each map generated by the various filters could then combined into one output feature vector.</b>\n",
    "    \n",
    "* Why might order of activations need to be retained?\n",
    "\n",
    "<b>Answer</b>:\n",
    "</br><b>Order is important as the position of a word in a sentence is important to its context. Sequence order is preserved in a CNN. </b>\n",
    "\n",
    "* In the code, we will add an additional row of zeros to represent the padded words. Will the zeros of the padded words be updated during back-prop? Why or why not?\n",
    "\n",
    "<b>Answer</b>:\n",
    "</br><b>No they will not be updated. Values are not updated during backpropogation, weights are. The weights connected to the zero padding will be updated as the padding is considered in the same filter as input values.</b>\n",
    "\n",
    "\n",
    "2 - https://arxiv.org/abs/1408.5882"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrDDcU5GGmoe"
   },
   "source": [
    "First, we will write code which can select k top elements in the order they appeared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8SViy9l_Gmog"
   },
   "outputs": [],
   "source": [
    "def k_max_pool(A,k):\n",
    "    \"\"\"\n",
    "    A = 2 dimensional array (assume that the length of last dimension of A will be always more than k)\n",
    "    k = number of elements.\n",
    "    Return: For every row of A, top k elements in the order they appear.\n",
    "    \"\"\"\n",
    "    assert len(A.get_shape())==2\n",
    "    def func(row):\n",
    "        \"\"\"\n",
    "        Hint : I used top_k and reverse.\n",
    "        I am not sure whether the order of the indices are retained when sorted = False in top_k. (did not find any documentation)\n",
    "        Therefore, I suggest that you sort the indices before selecting the elements from the array(Trick: use top_k again!)\"\"\"\n",
    "        ret_tensor = None\n",
    "        top_k = tf.nn.top_k(row, k = k, sorted=False, name=None)\n",
    "        sorted_indices = tf.nn.top_k(top_k.indices, k = k, sorted=False, name=None)\n",
    "        ordered_top_k_indices = tf.reverse(sorted_indices.values, [-1])\n",
    "        ret_tensor = tf.gather(row, ordered_top_k_indices)\n",
    "        return ret_tensor\n",
    "    return tf.map_fn(func,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MayRnDIAGmoi"
   },
   "outputs": [],
   "source": [
    "A = tf.placeholder(shape=[None,None],dtype=tf.float64)\n",
    "top = k_max_pool(A,5)\n",
    "sess = tf.Session()\n",
    "for i in range(1,6):\n",
    "    np.random.seed(5)\n",
    "    l = np.random.randn(i*10,i*10)\n",
    "    top_elements = sess.run(top,feed_dict={A:l})\n",
    "    l = l.tolist()\n",
    "    top_elements2 = np.array(map(lambda x: [x[i] for i in range(len(x)) if x[i]>sorted(x,reverse=True)[5]],l))\n",
    "    # Note that this test assumes that the 6th largest element and 5th largest element are different.\n",
    "    print(((top_elements - top_elements2)<10**-10).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KN66FnNcGmok"
   },
   "outputs": [],
   "source": [
    "def initializer(shape):\n",
    "    xavier = tf.contrib.layers.xavier_initializer(seed=1)\n",
    "    return xavier(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HeXmr83YGmom"
   },
   "outputs": [],
   "source": [
    "class CNN_simple:\n",
    "    def __init__(self,num_words,embedding_size = 30,span=2,k=5):\n",
    "        self.num_words = num_words\n",
    "\n",
    "        # The batch of text documents. Let's assume that it is always padded to length 100. \n",
    "        # We could use [None,None], but we'll use [None,100] for simplicity. \n",
    "        self.input = tf.placeholder(shape=[None,100],dtype=tf.int32)\n",
    "        self.expected_output = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "        \n",
    "\n",
    "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
    "        # Add an additional row of zeros to denote padded words.\n",
    "        self.embedding_matrix = None\n",
    "        \n",
    "        \n",
    "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size. \n",
    "        # Use embedding lookup\n",
    "        vectors = None # None x 100 x embedding_size\n",
    "        \n",
    "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
    "        # The convention is NHWC - None (Batch Size) x Height(Height of image) x Width(Width of image) x Channel(Depth - similar to RGB).\n",
    "        # For text, let's consider Height = 1, width = number of words, channel = embedding_size.\n",
    "        # Use expand-dims to modify. \n",
    "        vectors2d = None # None x 1 x 100 x embedding_size\n",
    "        \n",
    "        # Conv2d needs a filter bank.\n",
    "        # The dimensions of the filter bank = Height, Width, in-channels, out-channels(Number-of-Filters).\n",
    "        # We are creating a single filter of size = span. \n",
    "        # So, height = 1, width = span, in-channels = embedding_size ,out-channels = 1. \n",
    "        single_filter = tf.Variable(initializer((None, None, None, None)), name=\"filter\")  \n",
    "        bias = tf.Variable(0.0,name=\"bias\") # You need a bias for each filter.\n",
    "        conv_span = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=single_filter,\n",
    "            # Note that the first and last elements SHOULD be 1. \n",
    "            strides=[1, 1, 1, 1], \n",
    "            # This means that we are ok with input size being reduced during the process of convolution.\n",
    "            padding=\"VALID\"\n",
    "        ) # Shape = ?\n",
    "        \n",
    "        acts = tf.nn.leaky_relu(conv_span+bias)\n",
    "        \n",
    "        # Now, let us extract the top k activations. \n",
    "        # But, we need to first convert acts this into 2-dimensional.  \n",
    "        # Use tf.squeeze. Be sure to specify the squeeze-dimensions. \n",
    "        acts_2d = None\n",
    "        \n",
    "        # Use k_max_pool to extract top-k activations\n",
    "        input_fully_connected = k_max_pool(acts_2d,k) # None x k\n",
    "        \n",
    "        # Initialize the weight and bias needed for softmax classifier. \n",
    "        self.softmax_weight = None\n",
    "        self.softmax_bias = None\n",
    "        \n",
    "        # Write out the equation for computing the logits.\n",
    "        self.output = tf.nn.softmax(None, axis=1) # Shape = ?\n",
    "        \n",
    "        # Compute the cross-entropy cost. \n",
    "        # You might either sum or take mean of all the costs across all the examples. \n",
    "        # It is your choice as the test case is on Stochastic Training. \n",
    "        self.cost = None\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1),[-1,1]), tf.cast(self.expected_output, dtype=tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))        \n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def pad(self,data,pad_word,pad_length=100):\n",
    "        for datum in data:\n",
    "            datum.extend([pad_word]*(pad_length-len(datum)))\n",
    "        return data\n",
    "    \n",
    "    def train(self,train_data,test_data,train_targets,test_targets,batch_size=1,epochs=1,verbose=False):\n",
    "        sess = self.session\n",
    "        self.pad(train_data,self.num_words)\n",
    "        self.pad(test_data,self.num_words)\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(epochs):\n",
    "            cost_epoch = 0\n",
    "            c = 0\n",
    "            for datum,target in zip([train_data[i:i+batch_size] for i in range(0,len(train_data),batch_size)],\n",
    "                                   [train_targets[i:i+batch_size] for i in range(0,len(train_targets),batch_size)]):\n",
    "                _,cost = sess.run([self.train_op,self.cost],feed_dict={self.input:datum,self.expected_output:target})\n",
    "                cost_epoch += cost\n",
    "                c += 1\n",
    "                if c%100 == 0 and verbose:\n",
    "                    print(\"\\t{} batches finished. Cost : {}\".format(c,cost_epoch/c))\n",
    "            print(\"Epoch {}: {}\".format(epoch,cost_epoch/len(train_data)))\n",
    "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data,train_targets)))\n",
    "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data,test_targets)))\n",
    "    \n",
    "    def compute_accuracy(self,data,targets):\n",
    "        return self.session.run(self.accuracy,feed_dict={self.input:data,self.expected_output:targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KN8RoSLoGmoo"
   },
   "outputs": [],
   "source": [
    "c=CNN_simple(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qAcWbzyGmop"
   },
   "source": [
    "The expected output for the above snippet is\n",
    "<pre>\n",
    "Starting training...\n",
    "\t100 batches finished. Cost : 0.688363179564\n",
    "\t200 batches finished. Cost : 0.695461705327\n",
    "\t300 batches finished. Cost : 0.695902070602\n",
    "\t400 batches finished. Cost : 0.697339072227\n",
    "\t500 batches finished. Cost : 0.698220448136\n",
    "    ...\n",
    "Epoch 0: 0.675099702418\n",
    "\tTrain accuracy: 0.718958854675\n",
    "\tTest accuracy: 0.664322555065   \n",
    "</pre>\n",
    "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rtV-0gInGmoq"
   },
   "source": [
    "## ConvNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xi_un8GvGmor"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtrPWOepGmor"
   },
   "source": [
    "<img src=\"https://web.cs.dal.ca/~sastry/cnn.png\" style=\"height:40%;width:40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLMzzdMBGmos"
   },
   "source": [
    "Essentially, there are 2 kind of hyper-parameters - the filter size and number of filters of each size. In the image shown, there are 3 filter-sizes - 2,3,4 and number of filters of each size is 2. Once the convolution is obtained, 1-max pooling is done - it basically involves extracting 1 activation from the list of activations which is the maximum activation. The reason we need to do this is to construct the inputs to the softmax layer which are of a fixed size.\n",
    "Read more at https://arxiv.org/pdf/1510.03820.pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sqoRWB9wGmot"
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self,num_words,embedding_size = 30):\n",
    "        self.num_words = num_words\n",
    "\n",
    "        # The batch of text documents. Let's assume that it is always padded to length 100. \n",
    "        # We could use [None,None], but we'll use [None,100] for simplicity. \n",
    "        self.input = tf.placeholder(shape=[None,100],dtype=tf.int32)\n",
    "        self.expected_output = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "        \n",
    "\n",
    "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
    "        # Add an additional row of zeros to denote padded words.\n",
    "        self.embedding_matrix = None\n",
    "        \n",
    "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size. \n",
    "        # Use embedding lookup\n",
    "        vectors = None # None x 100 x embedding_size\n",
    "        \n",
    "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
    "        # The convention is NHWC - None (Batch Size) x Height(Height of image) x Width(Width of image) x Channel(Depth - similar to RGB).\n",
    "        # For text, let's consider Height = 1, width = number of words, channel = embedding_size.\n",
    "        # Use expand-dims to modify. \n",
    "        vectors2d = None # None x 1 x 100 x embedding_size\n",
    "        \n",
    "        # Create 50 filters with span of 3 words. You need 1 bias for each filter.\n",
    "        filter_tri = tf.Variable(initializer((None,None,None,None)), name=\"weight3\")  \n",
    "        bias_tri = tf.Variable(tf.zeros((None,None)), name=\"bias3\")  \n",
    "        conv1 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_tri,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = ?\n",
    "        A1 = tf.nn.leaky_relu(conv1+bias_tri)\n",
    "\n",
    "        # Create 50 filters with span of 4 words. You need 1 bias for each filter.\n",
    "        filter_4 = tf.Variable(initializer((None,None,None,None)), name=\"weight4\")  \n",
    "        bias_4 = tf.Variable(tf.zeros((None,None)), name=\"bias4\")\n",
    "        conv2 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_4,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = ?\n",
    "\n",
    "        A2 = tf.nn.leaky_relu(conv2+bias_4)\n",
    "\n",
    "        # Create 50 filters with span of 5 words. You need 1 bias for each filter.\n",
    "        filter_5 = tf.Variable(initializer((None,None,None,None)), name=\"weight5\")  \n",
    "        bias_5 = tf.Variable(tf.zeros((None,None)), name=\"bias5\")\n",
    "        conv3 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_5,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = ?\n",
    "\n",
    "        A3 = tf.nn.leaky_relu(conv3+bias_5)\n",
    "        \n",
    "        # Now extract the maximum activations for each of the filters. The shapes are listed alongside. \n",
    "        max_A1 = None  # None x 50\n",
    "        max_A2 = None  # None x 50\n",
    "        max_A3 = None  # None x 50\n",
    "        \n",
    "        concat = tf.concat([max_A1, max_A2, max_A3], axis=1) # None x 150\n",
    "        \n",
    "        # Initialize the weight and bias needed for softmax classifier. \n",
    "        self.softmax_weight = None\n",
    "        self.softmax_bias = None\n",
    "        \n",
    "        # Write out the equation for computing the logits.\n",
    "        self.output = tf.nn.softmax(None, axis=1) # Shape = ?\n",
    "        \n",
    "        # Compute the cross-entropy cost. \n",
    "        # You might either sum or take mean of all the costs across all the examples. \n",
    "        # It is your choice as the test case is on Stochastic Training. \n",
    "        self.cost = None\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1),[-1,1]), tf.cast(self.expected_output, dtype=tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def pad(self,data,pad_word,pad_length=100):\n",
    "        for datum in data:\n",
    "            datum.extend([pad_word]*(pad_length-len(datum)))\n",
    "        return data\n",
    "    \n",
    "    def train(self,train_data,test_data,train_targets,test_targets,batch_size=1,epochs=1,verbose=False):\n",
    "        sess = self.session\n",
    "        self.pad(train_data,self.num_words)\n",
    "        self.pad(test_data,self.num_words)\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(epochs):\n",
    "            cost_epoch = 0\n",
    "            c = 0\n",
    "            for datum,target in zip([train_data[i:i+batch_size] for i in range(0,len(train_data),batch_size)],\n",
    "                                   [train_targets[i:i+batch_size] for i in range(0,len(train_targets),batch_size)]):\n",
    "                _,cost = sess.run([self.train_op,self.cost],feed_dict={self.input:datum,self.expected_output:target})\n",
    "                cost_epoch += cost\n",
    "                c += 1\n",
    "                if c%100 == 0 and verbose:\n",
    "                    print(\"\\t{} batches finished. Cost : {}\".format(c,cost_epoch/c))\n",
    "            print(\"Epoch {}: {}\".format(epoch,cost_epoch/len(train_data)))\n",
    "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data,train_targets)))\n",
    "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data,test_targets)))\n",
    "    \n",
    "    def compute_accuracy(self,data,targets):\n",
    "        return self.session.run(self.accuracy,feed_dict={self.input:data,self.expected_output:targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dfYIkXOkGmov"
   },
   "outputs": [],
   "source": [
    "c=CNN(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhrJL02kGmow"
   },
   "source": [
    "The expected output for the above snippet is\n",
    "<pre>\n",
    "Starting training...\n",
    "\t100 batches finished. Cost : 0.697723334432\n",
    "\t200 batches finished. Cost : 0.69957424134\n",
    "\t300 batches finished. Cost : 0.697673715353\n",
    "\t400 batches finished. Cost : 0.692196451947\n",
    "\t500 batches finished. Cost : 0.693883402467\n",
    "    ...\n",
    "Epoch 0: 0.624233247656\n",
    "\tTrain accuracy: 0.828467607498\n",
    "\tTest accuracy: 0.736521303654   \n",
    "</pre>\n",
    "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ozMFo7kGmow"
   },
   "source": [
    "### Effect of Batch Size on Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tybsvV02Gmox"
   },
   "source": [
    "Study the effects of changing batch size. Just run the various experiments and observe the results (Run it in non-verbose mode). No need to make any comments here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9JjNaWPtGmoy"
   },
   "outputs": [],
   "source": [
    "c=CNN(len(vocab))\n",
    "print(\"Batch size 2:\")\n",
    "c.train(train_data,test_data,train_targets,test_targets, batch_size=2, epochs=1,verbose=False)\n",
    "print(\"Batch size 4:\")\n",
    "c.train(train_data,test_data,train_targets,test_targets, batch_size=4, epochs=1,verbose=False)\n",
    "print(\"Batch size 8:\")\n",
    "c.train(train_data,test_data,train_targets,test_targets, batch_size=8, epochs=1,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzOv1TG_Gmoz"
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVha1w_BGmo0"
   },
   "source": [
    "Add 2 functions - get_distance and get_most_similar to the CNN class (the big one). \n",
    "* get_distance(word1,word2) - should return the cosine distance between the 2 words.\n",
    "* get_most_similar(word) - should return top 10 most similar words to the word passed.\n",
    "\n",
    "Now, use the 2 functions to record the distances between a list of word-pairs as the training progresses. (One easy way to go about could be to save the embedding matrix in the hard-disk for every 5 updates.):\n",
    "* Study the distance between words of opposite sentiment as the training progresses. Ex: Good and Bad, Good and horrible, etc.\n",
    "* Study the distance between words of same sentiment. Ex: Good and Beautiful, Bad and Terrible, etc.\n",
    "* Study how the non-sentiment bearing words relate to each other. Ex: his, her, an, it, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zw4bN713Gmo0"
   },
   "outputs": [],
   "source": [
    "## Could not get model-saving stuff working. For examples sake return similarities from some randomly sampled words\n",
    "## Examples are done comparing values to the word 'horrible'\n",
    "## Both the distance and the similarity functions can be seen in the CNN\n",
    "c=CNN(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMOB3iO5ag_X"
   },
   "source": [
    "### Learnings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bj0s5x-FarR3"
   },
   "source": [
    "List out the observations and conclusions you made from the various experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TpC2Sjpfa4Zj"
   },
   "outputs": [],
   "source": [
    "# The experiments reveal some expected results but also show some hidden insights \n",
    "# into how the model classifies and what additional information it can reveal.\n",
    "\n",
    "# In studying the difference between words of opposite and the same sentiment based \n",
    "# off our understanding of the english language it can be seen that as the model \n",
    "# becomes better trained the words are seen to be more different or similar. Words like \n",
    "# horrible and good start with a large difference, but they end with an even larger\n",
    "# distance after additional training. The difference between similar words also\n",
    "# substancially drops as training continues.\n",
    "\n",
    "# Through experimentation of finding words which are similar to 'horrible' from a\n",
    "# randomly selected pool an interesting aspect of sentiment classification could \n",
    "# be seen. Some actors that are consistently in movies with more negative reviews\n",
    "# were classified as being a strong indicator of a negative sentiment. This can be\n",
    "# seen through the similarities to 'woody' and 'hennings' which are actor's names.\n",
    "\n",
    "# Another interesting result from similar words to 'horrible' is that features that\n",
    "# could make a movie less enjoyable were revealed. A common critisim of a film\n",
    "# appears to be that the plot has 'unanswered' questions or 'squanders' possibilites.\n",
    "\n",
    "# Finally, words from commonly negative phrases such as 'fingernails' on a chalkboard\n",
    "# were considered similar to the word 'horrible'."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Copy of A3.ipynb",
   "provenance": [
    {
     "file_id": "1Vg_mOVrg8tlz_ihKzNbZynKWVp4SmBZx",
     "timestamp": 1528208478572
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
